{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tóm tắt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng tôi đề xuất xây dựng hệ thống rút tin tức từ Internet và thống kê các từ khóa chính xuất hiện theo thời gian thực. Cơ sở dữ liệu căn bản của chúng tôi dùng là Postgresql, kết hợp với các tiện ích khác để xây dựng hệ thống lưu trữ phân tán và cơ sở dữ liệu chuỗi thời gian giúp đáp ứng được các yêu cầu cơ bản. Hệ thống được đề xuất tuy đơn giản nhưng có khả năng đáp ứng được các yêu cầu của dữ liệu lớn và dễ dàng mở rộng khi cần."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giới thiệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rút tin (Web scraping) là quá trình xử lý, rút trích thông tin tự động từ các trang trên Internet để tổng hợp và làm giàu nguồn dữ liệu cho chính ứng dụng hiện tại. Một hệ thống rút tin đơn giản bao gồm frontier chứa các url cần được xử lý. Các url sẽ được xử lý tuần tự hoặc theo một nguyên tắc nào đó và rút các nội dung từ thẻ html. Các nội dung sẽ được thêm vào một cơ sở dữ liệu lưu trữ cùng với các url mới được tiếp tục thêm vào frontier. \n",
    "\n",
    "CSDL quan hệ (SQL) được ra mắt từ rất sớm và đem lại nhiều lợi ích cho con người. Nhưng khi nhu cầu khai thác dữ liệu tăng cao, các CSDL phi quan hệ (NoSQL) khi đó ra đời như MapReduce (1), Google Bigtable (2), Cassandra (3), Mongodb (4),... đem lại nhiều tiện ích khiến chúng ta dần quên đi SQL trong các ứng dụng. Phải mất một thời gian, các CSDL quan hệ được mới tiếp tục được sử dụng nhiều trở lại bởi các dự án mã nguồn mở đã làm cho chúng ngày càng mềm dẻo. \n",
    "\n",
    "\n",
    "Trong nhiều trường hợp, việc sử dụng các NoSQL là không cần thiết và thậm chí còn gây lãng phí bởi các CSDL quan hệ ngày nay đã đáp ứng được các dữ liệu phi cấu trúc. Postgresql sử dụng những kiểu dữ liệu cơ bản của SQL thông thường cộng thêm một số kiểu dữ liệu phi cấu trúc làm cho nó trở nên tiện dụng. Ngoài ra, các tiện ích mở rộng cũng như bản quyền Postgres đã giúp cho nó phát triển nhanh và trở thành CSDL được yêu thích nhất năm 2019 (5). Chính vì các điểm trên, chúng tôi đề xuất sử dụng Postgresql làm cơ sở dữ liệu lưu trữ thông tin cho ứng dụng.  \n",
    "\n",
    "Các điểm nhấn của đồ án cuối kỳ như sau: \n",
    "\n",
    "1. Xây dựng hệ thống rút tin tức và thống kê từ khóa đơn giản nhưng đáp ứng được luồng dữ liệu đến cực lớn và có khả năng mở rộng linh hoạt  \n",
    "2. Đề xuất sử dụng cơ sở dữ liệu (CSDL) quan hệ và các tiện ích mở rộng để lưu trữ và thống kê dữ liệu thay cho các CSDL phi quan hệ (NoSQL)\n",
    "3. Triển khai các dịch vụ trên docker "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hệ thống "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng tôi đề xuất sử dụng Postgresql (Postgres), Redis, Kafka cùng một số tiện ích mở rộng của Postgres làm cơ sở xây dựng hệ thống. Hình bên dưới minh họa tóm tắt hệ thống chúng tôi xây dựng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/system.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hệ thống trên gồm hai thành phần chính. ```www``` có nhiệm vụ thu thập các url từ những trang web cho trước. Các url được chuẩn hóa và lưu trữ dưới dạng mã hóa sha1 trong Redis một bản duy nhất (với key là mã băm của url). Với những url mới, ```www``` đưa chúng vào Kafka và chờ chương trình rút nội dung, rút thực thể xử lý. \n",
    "\n",
    "Vì các bài báo mới không nhiều nên chúng tôi đưa chúng trực tiếp vào Postgres. Các keywords xuất hiện nhiều nên chúng tôi đưa chúng vào Kafka và dùng tiện ích mở rộng khác của Postgres là PipelineDB + PipelineKafka để đồng bộ số dữ liệu này. Các từ khóa sẽ được thống kê thông qua khung nhìn liên tục của PipelineDB được viết giống khung nhìn của SQL cơ bản. \n",
    "\n",
    "Để phân tán dữ liệu, chúng tôi sử dụng Citusdata với master và các worker node. Bảng lưu trữ dữ liệu sẽ được tạo thành bảng phân tán thông qua lệnh ```create_distributed_table``` của Citus. Vì phần cứng hạn chế nên chúng tôi chỉ thực hiện khởi tạo 1 master node và 3 worker tương ứng với master và slave 1 - 3. \n",
    "\n",
    "Chúng tôi thực hiện cài đặt các phần mềm cơ sở dữ liệu trực tiếp trên các slave master và slave 1 - 3. Các dịch vụ, mã nguồn khác được chạy độc lập trên container ở slave 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cài đặt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cài đặt docker và docker-compose trên slave 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cài đặt các gói cần thiết\n",
    "\n",
    "```sudo apt install apt-transport-https ca-certificates curl software-properties-common```\n",
    "\n",
    "Tiếp theo, cập nhật key của docker vào repository của ubuntu\n",
    "\n",
    "```curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -```\n",
    "\n",
    "Thêm vào apt source list\n",
    "\n",
    "```sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\"```\n",
    "\n",
    "Cập nhật repository \n",
    "\n",
    "```sudo apt-get update```\n",
    "\n",
    "Cài đặt docker community edition\n",
    "\n",
    "```sudo apt install -y docker-ce```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cài đặt Postgresql tại master và các slave còn lại "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thêm key postgresql \n",
    "\n",
    "```wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O- | sudo apt-key add -```\n",
    "\n",
    "Thêm vào source list \n",
    "\n",
    "```echo \"deb http://apt.postgresql.org/pub/repos/apt/ bionic-pgdg main\" | sudo tee /etc/apt/sources.list.d/postgresql.list```\n",
    "\n",
    "Cập nhật repository\n",
    "\n",
    "```sudo apt-get update```\n",
    "\n",
    "Cài đặt postges 11 \n",
    "\n",
    "```sudo apt-get install -y postgresql-11```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citusdata tại master và các slave còn lại "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thêm vào repository\n",
    "\n",
    "```curl https://install.citusdata.com/community/deb.sh | sudo bash```\n",
    "\n",
    "Cài Citus\n",
    "\n",
    "```sudo apt-get -y install postgresql-11-citus```\n",
    "\n",
    "Sau khi cài đặt xong Postgres và Citus ở các máy, tiếp tục thực hiện mở địa chỉ để các node Postgres giao tiếp với nhau. Thực hiện các bước sau trên tất cả các máy này: \n",
    "\n",
    "1. Chỉnh listen \n",
    "\n",
    "```sudo vim /etc/postgresql/11/main/postgresql.conf``` \n",
    "\n",
    "tìm dòng ```listen_addresses='127.0.0.1``` và thay bằng ```listen_addresses='*'```\n",
    "\n",
    "2. Tiếp theo thêm extension bằng cách thêm dòng  \n",
    "\n",
    "```shared_preload_libraries = 'citus'```\n",
    "\n",
    "3. Lưu và tiếp tục sửa \n",
    "\n",
    "```sudo vim /etc/postgresql/11/main/pg_hba.conf``` \n",
    "\n",
    "thay \n",
    "\n",
    "```host    all             all             127.0.0.1/32              md5```\n",
    "\n",
    "thành \n",
    "\n",
    "```host    all             all             0.0.0.0/0              trust```\n",
    "\n",
    "\n",
    "4. Lưu và chạy lệnh khởi động lại pg server \n",
    "\n",
    "```service postgresql restart``` \n",
    "\n",
    "** Kiểm tra xem pg còn hoạt động hay không \n",
    "\n",
    "\n",
    "```service postgresql status```\n",
    "\n",
    "và cluster \n",
    "\n",
    "```pg_lsclusters```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citusdata tại master node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chọn csdl và tạo extension bằng lệnh \n",
    "\n",
    "```CREATE EXTENSION citus;```\n",
    "\n",
    "Kết nối với các slave bằng lệnh \n",
    "\n",
    "```SELECT * from master_add_node(SLAVE_IP_ADDRESS_OR_NAME, BINDING_PORT);```\n",
    "\n",
    "ví dụ: \n",
    "\n",
    "```SELECT * from master_add_node('slave-1', 5432);```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cài PipelineDB tại master\n",
    "\n",
    "Thêm apt repository \n",
    "\n",
    "```curl -s http://download.pipelinedb.com/apt.sh | sudo bash```\n",
    "\n",
    "Cài đặt \n",
    "\n",
    "```sudo apt-get install pipelinedb-postgresql-11```\n",
    "\n",
    "Chỉnh preload của pg trong tập tin ```sudo vim /etc/postgresql/11/main/postgresql.conf```\n",
    "\n",
    "```shared_preload_libraries = 'citus,pipelinedb'```\n",
    "\n",
    "và \n",
    "\n",
    "```max_worker_processes = 128```\n",
    "\n",
    "khởi động lại Postgres server bằng lệnh ```service postgresql restart``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cài kafka-pipeline \n",
    "\n",
    "Thực hiện lần lượt các lệnh sau:\n",
    "\n",
    "```sudo apt-get install git gcc g++ zlib1g-dev```\n",
    "\n",
    "```git clone https://github.com/edenhill/librdkafka.git```\n",
    "\n",
    "```git clone https://github.com/pipelinedb/pipeline_kafka.git```\n",
    "\n",
    "```cd librdkafka```\n",
    "\n",
    "```./configure --prefix=/usr ```\n",
    "\n",
    "```make && sudo make install```\n",
    "\n",
    "\n",
    "```cd pipeline_kafka```\n",
    "\n",
    "```./configure ```\n",
    "\n",
    "```make && sudo make install```\n",
    "\n",
    "Cuối cùng, chỉnh preload trong postgresql.conf và khởi động lại server \n",
    "\n",
    "```shared_preload_libraries = 'citus,pipelinedb,kafka-pipeline'```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Khởi động các container app tại slave 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di chuyển đến thư mục project và gõ lệnh:\n",
    "\n",
    "```sudo docker-compose up -d```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi tiết hiện thực"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng tôi hiện thực chương trình bằng python3.6 được chạy trên ubuntu 18.04. Chúng tôi thực hiện kết nối với Postgresql bằng ```sqlalchemy```, thực hiện các thao tác với Kafka bằng ```kafka-python```. \n",
    "\n",
    "Các bài báo được rút từ 150 nguồn tiếng Anh (xem ```pages.txt```). Chúng tôi thực hiện rút tiêu đề, tóm tắt, nội dung, ngày đăng, tác giả từ bài báo thông qua thư viện ```newspaper3k```. Từ tiêu đề và nội dung, chúng tôi tiếp tục thực hiện rút các thực thể, hay còn gọi là từ khóa bằng thư viện ```nltk```. Các từ khóa sẽ được thống kê số lượng xuất hiện mỗi ngày bằng khung nhìn liên tục của PipelineDB từ bảng stream. Khung nhìn liên tục được viết bằng lệnh sql: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```CREATE VIEW keyword_stats WITH (action=materialize) AS\n",
    "SELECT data.keyword, DATE(data.created_date) AS date, COUNT(DATE(data.created_date)) AS total\n",
    "FROM public.data\n",
    "GROUP BY data.keyword, DATE(data.created_date)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mã nguồn đồ án nằm trong thư mục ```src``` gồm các tập tin:\n",
    "\n",
    "1. ```arguments.py``` chứa các tham số khi chạy chương trình như sau: \n",
    "\n",
    "```--kafka_host``` địa chỉ host và port để kết nối đến kafka server. Ví dụ: 127.0.0.1:9092 \n",
    "\n",
    "```--kafka_link_topic``` topic chứa các liên kết, dùng như hàng đợi chuẩn bị cho rút nội dung \n",
    "\n",
    "```--kafka_keyword_topic``` topic chứa các keyword sau khi đã được rút nội dung. PipelineDB sẽ liên kết đến topic này và tự động stream dữ liệu và bảng chuẩn bị cho thống kê \n",
    "\n",
    "```--kafka_default_group``` nhóm làm việc của kafka consumer, dùng để theo dõi những offset đã duyệt qua để tiếp tục chạy phòng trường hợp consumer bị ngắt \n",
    "\n",
    "```--redis_host``` địa chỉ host của redis database \n",
    "\n",
    "```--redis_port``` port redis \n",
    "\n",
    "```--redis_db``` số thứ tự của redis database. Mặc định ở đây là $1$ \n",
    "\n",
    "```--redis_password``` mật khẩu của redis server \n",
    "\n",
    "```--pg_host``` địa chỉ host của postgresql\n",
    "\n",
    "```--pg_port``` port postgres\n",
    "\n",
    "```--pg_user``` user được quyền truy cập vào database \n",
    "\n",
    "```--pg_password``` password của user \n",
    "\n",
    "```--pg_db``` database cần được truy cập \n",
    "\n",
    "```--pg_relation``` tên bảng chứa dữ liệu tin được rút về \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ```helper.py``` hiện thực các phương thức như kết nối database, database model, rút thực thể (keyword) từ text, mã hóa url, chuẩn hóa url \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ```logger.py``` ghi log của chương trình "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ```visitor.py``` lấy các link nội dung từ url cho trước và gởi vào Kafka "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ```scraper.py``` rút nội dung từ các link trong Kafka topic. Sau khi thực hiện xong, nội dung bài báo sẽ được gởi thẳng vào Postgresql. Các keywords được gởi vào PipelineDB thông qua Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Ví dụ chạy xem trong tập tin ```runvisitor.sh``` và ```runscraper.sh```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tham khảo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cài đặt docker: https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04\n",
    "\n",
    "Docker Kafka và Zookeeper: https://github.com/wurstmeister/kafka-docker\n",
    "\n",
    "Docker Redis: https://hub.docker.com/_/redis\n",
    "\n",
    "Cài đặt Postgres 11: https://www.itzgeek.com/how-tos/linux/ubuntu-how-tos/how-to-install-postgresql-10-on-ubuntu-18-04-lts.html\n",
    "\n",
    "Citus data: http://docs.citusdata.com/\n",
    "\n",
    "PipelineDB: http://docs.pipelinedb.com/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
